# BobaTalkie - Claude Development Context

## Claude Code Workflow
This project uses Claude Code with automatic CLAUDE.md loading:
- ‚úÖ **Auto-context**: Claude Code automatically reads this file on startup
- ‚úÖ **Real-time updates**: Changes to this file are immediately available
- ‚úÖ **Multi-tool execution**: Can run multiple commands in parallel
- ‚úÖ **Phoenix integration**: Understands Elixir/Phoenix toolchain
- üîÑ **Self-updating**: This file evolves with the codebase

## Current Status: VOICE-CONTROLLED GAME FUNCTIONAL ‚úÖ
**Phase 1 MVP Complete**: Full voice-controlled 2D grid navigation  
**Ready for Production**: Deepgram API integration for all browsers

## Project Summary
Voice-controlled language learning game built with Phoenix LiveView. Players speak commands to navigate a 2D grid world, learning pronunciation and vocabulary through immersive gameplay.

## Development Focus
**Phase 1 - Current Sprint**: Solo mode with 2D grid navigation
- ‚úÖ Project setup and architecture
- üîÑ Index/landing page with voice setup  
- üîÑ 2D grid map system (0s=walls, 1s=walkable)
- ‚è≥ Player movement via voice commands
- ‚è≥ Voice pipeline integration (<500ms target)

## Tech Stack
- **Backend**: Elixir/Phoenix with LiveView
- **Real-time**: Phoenix Channels over WebSockets
- **Voice**: Deepgram API for streaming ASR
- **Frontend**: LiveView + PWA capabilities
- **Deployment**: Fly.io

## Game Architecture (2D Grid System)

### Map Structure
```
Grid: 10x10 2D array
0 = wall/obstacle (black)
1 = walkable space (white/green)  
2 = player position (blue)
3 = interactive objects (yellow)

Example:
[1,1,1,0,0,0,1,1,1,1]
[1,2,1,0,3,0,1,1,1,1]  # 2=player, 3=item
[1,1,1,0,0,0,1,1,1,1]
```

### Code Architecture
```
lib/boba_talkie/
‚îú‚îÄ‚îÄ game/
‚îÇ   ‚îú‚îÄ‚îÄ world.ex        # 2D grid state management
‚îÇ   ‚îú‚îÄ‚îÄ player.ex       # Player position and actions  
‚îÇ   ‚îú‚îÄ‚îÄ commands.ex     # Voice command parsing
‚îÇ   ‚îî‚îÄ‚îÄ map_loader.ex   # Load/generate grid maps
‚îú‚îÄ‚îÄ voice/
‚îÇ   ‚îú‚îÄ‚îÄ recognizer.ex   # Deepgram integration
‚îÇ   ‚îî‚îÄ‚îÄ processor.ex    # Command validation
‚îî‚îÄ‚îÄ application.ex

lib/boba_talkie_web/
‚îú‚îÄ‚îÄ live/
‚îÇ   ‚îú‚îÄ‚îÄ game_live.ex    # Main game LiveView
‚îÇ   ‚îî‚îÄ‚îÄ index_live.ex   # Landing page LiveView  
‚îî‚îÄ‚îÄ components/
    ‚îî‚îÄ‚îÄ grid_component.ex # 2D grid renderer
```

### Voice Commands (Grid Navigation)
- "go north" ‚Üí player.y -= 1
- "go south" ‚Üí player.y += 1  
- "go east" ‚Üí player.x += 1
- "go west" ‚Üí player.x -= 1
- "look around" ‚Üí describe surrounding cells

## Current Implementation Status
- ‚úÖ Phoenix project with LiveView setup
- ‚úÖ Folder structure and comprehensive documentation
- ‚úÖ Index page with microphone test and game intro
- ‚úÖ 2D grid world system (10x10 with walls, paths, items)
- ‚úÖ Player movement logic with collision detection
- ‚úÖ Game LiveView with real-time grid display
- ‚úÖ Test controls for movement (arrow buttons)
- üîÑ Next: Voice capture integration
- ‚è≥ Pending: Deepgram ASR for voice recognition

## Ready to Test
Run `mix phx.server` and visit:
- http://localhost:4000 ‚Üí Landing page with mic test
- http://localhost:4000/game ‚Üí 2D grid game (use test buttons to move)

## Development Commands (Makefile)
```bash
# Quick start
make help          # Show all available commands  
make setup         # First-time project setup
make server        # Start development server (alias: make dev, make start)

# Development workflow  
make test          # Run all tests (alias: make t)
make test-watch    # Run tests in watch mode (alias: make tw)
make format        # Format code (alias: make fmt, make f)
make lint          # Run linter and static analysis

# Utilities
make shell         # Start IEx shell with project loaded
make doctor        # Check project health and versions
make clean         # Clean all build artifacts

# Direct Phoenix commands (if needed)
mix phx.server     # Start server
mix test           # Run tests  
mix deps.get       # Install dependencies
```

## Key Phoenix Files
- `lib/boba_talkie_web/endpoint.ex` - WebSocket and LiveView configuration
- `lib/boba_talkie_web/router.ex` - Route definitions
- `assets/js/app.js` - Frontend JavaScript entry point

## Voice Processing Flow
1. Browser captures audio via MediaRecorder
2. Stream audio chunks to Phoenix Channel
3. Forward to Deepgram for real-time ASR
4. Parse commands and update game state
5. Send feedback to LiveView for UI update

## Performance Targets
- Voice-to-action latency: <500ms end-to-end
- Audio streaming: 200-300ms chunks
- Game state updates: <100ms via PubSub

## Development Guidelines
- Prioritize solo mode functionality first
- Maintain real-time responsiveness
- Follow Elixir/Phoenix conventions
- Keep voice processing in memory only
- Optimize for mobile PWA experience

## Architecture Rules

### Modular File Organization
**Rule**: For each file when implementing significant functionality, if relevant, create a directory with the name of file followed by "modules" (e.g., `file_namemodules/`) and import it in the parent directory for better architecture.

**Examples**:
```
lib/boba_talkie/game/
‚îú‚îÄ‚îÄ world.ex                    # Main module
‚îú‚îÄ‚îÄ worldmodules/              # Modular components
‚îÇ   ‚îú‚îÄ‚îÄ grid_renderer.ex       # Grid rendering logic
‚îÇ   ‚îú‚îÄ‚îÄ collision_detector.ex  # Collision detection
‚îÇ   ‚îú‚îÄ‚îÄ map_generator.ex       # Map generation utilities
‚îÇ   ‚îî‚îÄ‚îÄ position_calculator.ex # Position math
‚îî‚îÄ‚îÄ player.ex

lib/boba_talkie/voice/
‚îú‚îÄ‚îÄ deepgram.ex                # Main API client
‚îú‚îÄ‚îÄ deepgrammodules/           # Modular components
‚îÇ   ‚îú‚îÄ‚îÄ audio_processor.ex     # Audio format handling
‚îÇ   ‚îú‚îÄ‚îÄ response_parser.ex     # API response parsing
‚îÇ   ‚îú‚îÄ‚îÄ streaming_client.ex    # Real-time streaming
‚îÇ   ‚îî‚îÄ‚îÄ error_handler.ex       # Error management
‚îî‚îÄ‚îÄ recognizer.ex

lib/boba_talkie_web/live/
‚îú‚îÄ‚îÄ game_live.ex              # Main LiveView
‚îú‚îÄ‚îÄ game_livemodules/         # LiveView modules
‚îÇ   ‚îú‚îÄ‚îÄ voice_handlers.ex     # Voice event handlers
‚îÇ   ‚îú‚îÄ‚îÄ movement_handlers.ex  # Movement logic
‚îÇ   ‚îú‚îÄ‚îÄ ui_helpers.ex         # Template helpers
‚îÇ   ‚îî‚îÄ‚îÄ state_manager.ex      # State management
‚îî‚îÄ‚îÄ index_live.ex
```

**When to Apply**:
- ‚úÖ File > 200 lines
- ‚úÖ Multiple distinct responsibilities
- ‚úÖ Complex logic that can be separated
- ‚úÖ Reusable components across modules
- ‚ùå Simple, single-purpose modules
- ‚ùå Files with minimal logic

### Debug Logging Configuration
**Rule**: Environment-based debug logging for production and development with configurable error printing.

**Environment Variables**:
```bash
# .env file
DEBUG_ENABLED=true          # Enable/disable debug logging
DEBUG_LEVEL=debug           # debug, info, warn, error
PRINT_ERRORS=true           # Print detailed errors
VOICE_DEBUG=true            # Voice-specific debugging
GAME_DEBUG=true             # Game logic debugging
```

**Implementation**:
- Use environment-controlled logging throughout codebase
- Separate debug levels for different subsystems
- Production-safe error handling with optional detailed output
- Performance-conscious debug statements

## Next Steps
1. Set up Deepgram integration for voice recognition
2. Create basic game engine structure
3. Implement voice command processing pipeline
4. Configure PWA for mobile-first experience
5. Set up development environment with testingyour_deepgram_api_key_here
